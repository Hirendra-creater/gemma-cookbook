{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJToYvrm_DhY"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Qa-E-1FN-9NY"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KiptfU5_Gqb"
      },
      "source": [
        "# In-context Learning with Gemma 3\n",
        "\n",
        "Large language models can learn new tasks or follow instructions through **in-context learning**, where examples within the input prompt guide them. This differs from traditional fine-tuning, which updates the model's parameters. Instead, it uses the model's existing knowledge and pattern recognition.\n",
        "\n",
        "The input prompt typically shows examples of the desired input-output format or task. This context allows the model to understand and respond appropriately to new, unseen inputs within the same prompt. The quality and relevance of these examples, along with the model's overall capacity, influence its effectiveness.\n",
        "\n",
        "Gemma 3 offers a much larger context window compared to its predecessor, with the **1B** parameter model supporting **32k tokens** and the **4B, 12B and 27B** models handling up to **128k tokens**, up from the previous 8k token limit.\n",
        "\n",
        "The larger context windows in models like Gemma 3 are particularly beneficial for in-context learning, as they allow for more examples and more complex instructions to be included within a single prompt, potentially leading to improved performance on a wider range of tasks without explicit fine-tuning.\n",
        "\n",
        "In this notebook, we'll apply in-context learning to replicate the result of our previous \"[Translator of Old Korean Literature](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_2]Translator_of_Old_Korean_Literature.ipynb)\" fine-tuning example.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_3]In-context_Learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AAt2yaXDzab"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Select the Colab runtime\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the Gemma model:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **▾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **T4 GPU**.\n",
        "\n",
        "\n",
        "### Gemma setup on Kaggle\n",
        "To complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run the Gemma 3 1B or 4B model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITz3QOZ3ENSV"
      },
      "source": [
        "### Set environment variables\n",
        "\n",
        "Set environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2-y_zXSERrY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata # `userdata` is a Colab API.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F_FMQ_aEZFr"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "This tutorial uses the [Gemma library](https://gemma-llm.readthedocs.io/) for JAX. Gemma library is a Python package built as an extension of [JAX](https://github.com/jax-ml/jax), letting you use the performance advantages of the JAX framework with dramatically less code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxJwGxdYEUVy"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install -q git+https://github.com/google-deepmind/gemma.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeMXpChgE55T"
      },
      "source": [
        "## Load and prepare the Gemma model\n",
        "\n",
        "1. Load the Gemma model with [`kagglehub.model_download`](https://github.com/Kaggle/kagglehub/blob/bddefc718182282882b72f814d407d89e5d178c4/src/kagglehub/models.py#L12), which takes three arguments:\n",
        "\n",
        "- `handle`: The model handle from Kaggle\n",
        "- `path`: (Optional string) The local path\n",
        "- `force_download`: (Optional boolean) Forces to re-download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLHAEX4pFELn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GEMMA_PATH: /kaggle/input/gemma-3/flax/gemma3-1b-it/1\n",
            "CKPT_PATH: /kaggle/input/gemma-3/flax/gemma3-1b-it/1/gemma3-1b-it\n",
            "TOKENIZER_PATH: /kaggle/input/gemma-3/flax/gemma3-1b-it/1/tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "# @markdown ---\n",
        "# @markdown Choose Gemma model\n",
        "GEMMA_VARIANT = 'gemma3-1b-it' # @param ['gemma3-1b-it', 'gemma3-4b-it'] {type:\"string\"}\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "GEMMA_PATH = kagglehub.model_download(f'google/gemma-3/flax/{GEMMA_VARIANT}')\n",
        "# example: download tokenizer only, force re-download\n",
        "#GEMMA_PATH = kagglehub.model_download(f'google/gemma-3/flax/{GEMMA_VARIANT}', path=\"tokenizer.model\", force_download=True)\n",
        "print('GEMMA_PATH:', GEMMA_PATH)\n",
        "\n",
        "import os\n",
        "CKPT_PATH = os.path.join(GEMMA_PATH, GEMMA_VARIANT)\n",
        "TOKENIZER_PATH = os.path.join(GEMMA_PATH, 'tokenizer.model')\n",
        "print('CKPT_PATH:', CKPT_PATH)\n",
        "print('TOKENIZER_PATH:', TOKENIZER_PATH)\n",
        "\n",
        "from gemma import gm\n",
        "\n",
        "if \"gemma3-1b\" in GEMMA_VARIANT:\n",
        "    model = gm.nn.Gemma3_1B()\n",
        "elif \"gemma3-4b\" in GEMMA_VARIANT:\n",
        "    model = gm.nn.Gemma3_4B()\n",
        "else:\n",
        "    raise ValueError(f\"Unknown GEMMA_VARIANT: {GEMMA_VARIANT}\")\n",
        "\n",
        "params = gm.ckpts.load_params(CKPT_PATH)\n",
        "tokenizer = gm.text.Gemma3Tokenizer(TOKENIZER_PATH)\n",
        "\n",
        "# @markdown Adjust cache size for in-context learning\n",
        "CACHE_LENGTH = 12*1024 # @param\n",
        "# @markdown Number of of tokens to generate output\n",
        "MAX_NEW_TOKENS = 256 # @param\n",
        "# @markdown ---\n",
        "\n",
        "sampler = gm.text.Sampler(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_length=CACHE_LENGTH,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbOOnAOEm09"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "Here's [the dataset](https://huggingface.co/datasets/bebechien/HongGildongJeon) from Hong Gildong jeon (Korean: 홍길동전), which is a Korean novel written during the Joseon Dynasty. The [original source](https://ko.wikisource.org/wiki/%ED%99%8D%EA%B8%B8%EB%8F%99%EC%A0%84_36%EC%9E%A5_%EC%99%84%ED%8C%90%EB%B3%B8) is in public domain. You will use a [modern translation](https://ko.wikisource.org/wiki/%ED%99%8D%EA%B8%B8%EB%8F%99%EC%A0%84_36%EC%9E%A5_%EC%99%84%ED%8C%90%EB%B3%B8/%ED%98%84%EB%8C%80%EC%96%B4_%ED%95%B4%EC%84%9D) in a [creative commons license](https://creativecommons.org/licenses/by-sa/4.0/), translated by `직지프로`.\n",
        "\n",
        "To simplify the task, you will adopt the following structure for fine-tuning the model. The model will generate contemporary Korean text based on the user's input in [Early Hangul](https://en.wikipedia.org/wiki/Origin_of_Hangul).\n",
        "\n",
        "```\n",
        "<start_of_turn>user\\n\n",
        "됴션국셰둉ᄃᆡ왕즉위십오연의홍희문밧긔ᄒᆞᆫᄌᆡ상이잇스되<end_of_turn>\\n\n",
        "<start_of_turn>model\\n\n",
        "조선국 세종대왕 즉위 십오년에 홍회문 밖에 한 재상이 있으되,<end_of_turn>\n",
        "```\n",
        "\n",
        "> NOTE: korean text means, In the fifteenth year of the reign of King Sejong of Joseon, there was a prime minister outside Honghoemun Gate.\n",
        "\n",
        "You'll start by loading the complete 'train' dataset and converting its features to NumPy arrays. Then you'll use few-shot prompting to include the dataset as an in-context learning example, utilizing the previously defined variables `in_context_limit = CACHE_LENGTH - MAX_NEW_TOKENS`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1acXTcsEwGv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['original', 'modern translation'],\n",
            "    num_rows: 447\n",
            "})\n",
            "11733\n",
            "<start_of_turn>user\n",
            "됴션국셰둉ᄃᆡ왕즉위십오연의홍희문밧긔ᄒᆞᆫᄌᆡ상이잇스되셩은홍이요명은문이니위인이쳥염강직ᄒᆞ여덩망이거록ᄒᆞ니당셰의영웅이라일직용문의올나벼살이할림의쳐ᄒᆞ엿더니명망이됴졍의읏듬되ᄆᆡ젼하그덕망을승이녀긔ᄉᆞ벼살을도도와이조판셔로좌으졍을ᄒᆞ이시니승상이국은을감동ᄒᆞ야갈츙보국ᄒᆞ니ᄉᆞ방의일이업고도젹이업스ᄆᆡ시화연풍ᄒᆞ여나라이ᄐᆡ평ᄒᆞ더라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "조선국 세종대왕 즉위 십오년에 홍회문 밖에 한 재상이 있으되, 성은 홍이요, 명은 문이니, 위인이 청렴강직하여 덕망이 거룩하니 당세의 영웅이라. 일찍 용문에 올라 벼슬이 한림에 처하였더니 명망이 조정의 으뜸 되매, 전하 그 덕망을 승히 여기사 벼슬을 돋우어 이조판서로 좌의정을 하게 하시니, 승상이 국은을 감동하여 갈충보국하니 사방에 일이 업고 도적이 없으매 시화연풍하여 나라가 태평하더라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "일일은승상난간의비겨잠ᄀᆞᆫ조의더니ᄒᆞᆫ풍이긜을인도ᄒᆞ여ᄒᆞᆫ고듸다다르니쳥산은암암ᄒᆞ고녹슈난양양ᄒᆞᆫ듸셰류쳔만ᄀᆞ지녹음이파ᄉᆞᄒᆞ고황금갓ᄐᆞᆫᄭᅬᄭᅩ리난춘흥을희롱ᄒᆞ여냥뉴간의왕ᄂᆡᄒᆞ며긔화요초만발ᄒᆞᆫᄃᆡ쳥학ᄇᆡᆨ학이며비취공작이춘광을ᄌᆞ랑ᄒᆞ거날승상이경물을귀경ᄒᆞ며졈졈드러가니만쟝졀벽은하날의다엇고구뷔구뷔벽계슈난골골이폭포되어오운이어러엿난ᄃᆡ길이ᄭᅳᆫ쳐갈바을모로더니문득쳥용이물결을혜치고머리을드러고함ᄒᆞ니산학이믄허지난듯ᄒᆞ더니그용이입을버리고긔운을토ᄒᆞ여승상의입으로드러뵈거날ᄭᆡ다르니평ᄉᆡᆼᄃᆡ몽이라ᄂᆡ염의혜아리되피련군ᄌᆞ을나희리라ᄒᆞ여즉시ᄂᆡ당의드러ᄀᆞ시비을믈이치고부인을익그러취침코져ᄒᆞ니부인이졍ᄉᆡᆨ왈승상은국지ᄌᆡ상이라쳬위존즁ᄒᆞ시거날ᄇᆡᆨ쥬의졍실의드러와노류장화갓치ᄒᆞ시니ᄌᆡ상의쳬면이어ᄃᆡ잇난잇ᄀᆞ승상이ᄉᆡᆼ각ᄒᆞ신직말ᄉᆞᆷ은당연ᄒᆞ오나ᄃᆡ몽을허송할가ᄒᆞ야몽ᄉᆞ을이르지아니ᄒᆞ지고연ᄒᆞ여간쳥ᄒᆞ시니부인이옷슬ᄯᅥᆯ치고밧그로나가시니승상이무류ᄒᆞ신즁의부인의도도ᄒᆞᆫ고집을ᄋᆡ달나무슈히ᄎᆞ탄ᄒᆞ시고외당으로나오시니마ᄎᆞᆷ시비춘셤이상을드리거날좌우고요ᄒᆞ믈인ᄒᆞ여춘셤을잇글고원앙지낙을일의시니져긔울화을더르시나심ᄂᆡ의못ᄂᆡ한탄ᄒᆞ시더라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "일일은 승상 난간에 비겨 잠깐 졸더니, 한풍이 길을 인도하여 한 곳에 다다르니, 청산은 암암하고 녹수는 양양한데 세류 천만 가지 녹음이 파사하고, 황금 같은 꾀꼬리는 춘흥을 희롱하여 양류간에 왕래하며 기화요초 만발한데, 청학 백학이며 비취 공작이 춘광을 자랑하거늘, 승상이 경물을 구경하며 점점 들어가니, 만장절벽은 하늘에 닿았고, 굽이굽이 벽계수는 골골이 폭포되어 오운이 어리었는데, 길이 끊어져 갈 바를 모르더니, 문득 청룡이 물결을 헤치고 머리를 들어 고함 하니 산학이 무너지는 듯하더니, 그 용이 입을 벌리고 기운을 토하여 승상의 입으로 들어오거늘, 깨달으니 평생 대몽이라.\n",
            "내염에 헤아리되 \"필연 군자를 낳으리라.\" 하여, 즉시 내당에 들어가 시비를 물리치고 부인을 이끌어 취침코자 하니, 부인이 정색 왈,\n",
            "\"승상은 국지재상이라, 체위 존중하시거늘 백주에 정실에 들어와 노류장화같이 하시니 재상의 체면이 어디에 있나이까?\"\n",
            "승상이 생각하신 즉, 말씀은 당연하오나 대몽을 허송할까 하여 몽사를 이르지 아니하시고 연하여 간청하시니, 부인이 옷을 떨치고 밖으로 나가시니, 승상이 무료하신 중에 부인의 도도한 고집을 애달아 무수히 차탄하시고 외당으로 나오시니, 마침 시비 춘섬이 상을 드리거늘, 좌우 고요함을 인하여 춘섬을 이끌고 원앙지낙을 이루시니 적이 울화를 덜으시나 심내에 못내 한탄하시더라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "춘셤이비록쳔인이나ᄌᆡ덕이순직ᄒᆞᆫ지라불의예승상으위엄으로친근ᄒᆞ시니감이위령치못ᄒᆞ여순종ᄒᆞᆫ후로난그날븟텀즁문밧긔나지아니ᄒᆞ고ᄒᆡᆼ실을닥그니그달봇텀ᄐᆡ긔잇셔십ᄉᆡᆨ이당ᄒᆞᄆᆡ거쳐ᄒᆞ는방의오ᄉᆡᆨ운무영농ᄒᆞ며향ᄂᆡ긔히ᄒᆞ더니혼미즁의ᄒᆡᄐᆡᄒᆞ니일ᄀᆡ긔남ᄌᆞ라ᄉᆞᆷ일후의승상이드러와보시니일변긧거오나그쳔ᄉᆡᆼ되믈앗긔시더라일홈을길동이라ᄒᆞ니라이아희졈졈ᄌᆞ라ᄆᆡ긔골이비상ᄒᆞ여ᄒᆞᆫ말을드르면열말을알고ᄒᆞᆫ번보면모로거시업더라일일은승상이길동을다리고ᄂᆡ당의드러ᄀᆞ부인을ᄃᆡᄒᆞ야탄식왈이아히비록영웅이오나쳔ᄉᆡᆼ이라무엇싀쓰리요원통할ᄉᆞ부인의고집이여후회맛급이로소이다부인이그연고을믓ᄌᆞ오니승상이양미을빈츅ᄒᆞ여왈부인이젼일의ᄂᆡ말을드르시던들이아히부인복즁의낫슬낫다엇지쳔ᄉᆡᆼ이되리요인ᄒᆞ여몽ᄉᆞ얼셜화ᄒᆞ시니부인이츄연왈ᄎᆞ역쳔슈오니엇지일력으로ᄒᆞ오릿ᄀᆞ<end_of_turn>\n",
            "<start_of_turn>model\n",
            "춘섬이 비록 천인이나 재덕이 순직한지라, 불의에 승상의 위엄으로 친근하시니 감이 위령치 못하여 순종한 후로는 그날부터 중문 밖에 나지 아니하고 행실을 닦으니 그달부터 태기있어 십삭이 당하매 거처하는 방에 오색운무 영롱하며 향내 기이하더니, 혼미중에 해태하니 일개 기남자라. 삼일 후에 승상이 들어와 보시니 일변 기꺼우나 그 천생됨을 아끼시더라. 이름을 길동이라 하니라.\n",
            "이 아이 점점 자라매 기골이 비상하여 한 말을 들으면 열 말을 알 고, 한 번 보면 모르는 것이 없더라. 일일은 승상이 길동을 데리고 내당에 들어가 부인을 대하여 탄식 왈,\n",
            "\"이 아이 비록 영웅이나 천생이라 무엇에 쓰리오. 원통하도다. 부인의 고집이여, 후회막급이로소이다.\"\n",
            "부인이 그 연고를 묻자오니, 승상이 양미를 빈축하여 왈,\n",
            "\"부인이 전일에 내 말을 들으셨던들 이 아이 부인 복중에 낳을 것을 어찌 천생이 되리요.\"\n",
            "인하여 몽사를 설화하시니, 부인이 추연 왈,\n",
            "\"차역 천수오니 어찌 인력으로 하오리까.\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "셰월이여류ᄒᆞ야길동의나히팔셰라상하다아니층찬ᄒᆞ리업고ᄃᆡ감도ᄉᆞ랑ᄒᆞ시나길동은가ᄉᆞᆷ의원한이부친을부친이라못ᄒᆞ고형을형이라부르지못ᄒᆞᄆᆡ스ᄉᆞ로쳔ᄉᆡᆼ되물자탓ᄒᆞ더니츄칠월망일의명월을ᄃᆡᄒᆞ야졍하의ᄇᆡ회ᄒᆞ더니츄풍은삽삽ᄒᆞ고긔러긔우난소ᄅᆡ은ᄉᆞᄅᆞᆷ의외로은심ᄉᆞ을돕ᄂᆞᆫ지라홀노탄식ᄒᆞ여왈ᄃᆡ장부세상의나ᄆᆡ공ᄆᆡᆼ의도학을ᄇᆡ화츌장입상ᄒᆞ여ᄃᆡ장인슈을요하의ᄎᆞ고ᄃᆡ장단의노피안ᄌᆞ쳔병만마을지위즁의너허두고남으로초를치고북으로즁원을뎡ᄒᆞ며셔으로촉을쳐ᄉᆞ업을일운후의얼골을긔린각의빗ᄂᆡ고일홈을후셰예유젼ᄒᆞ미ᄃᆡ장부의ᄯᅥᄯᅥᄒᆞᆫ일이라옛ᄉᆞᄅᆞᆷ이이르긔를왕후장상이씨업다ᄒᆞ엿시니날을두고이르민ᄀᆞ셰상ᄉᆞᄅᆞᆷ이갈관박이라도부형을부형이라ᄒᆞ되나ᄂᆞᆫ홀노그러치못ᄒᆞ니이어인인ᄉᆡᆼ으로그러ᄒᆞᆫ고울억ᄒᆞᆫ마음을것잡지못ᄒᆞ야칼을잡고월하의츔을츄며장ᄒᆞᆫ긔운이기지못ᄒᆞ더니이ᄯᆡ승상이명월을ᄉᆞ랑ᄒᆞ야창을열고비겻더니길동의거동을보시고놀ᄂᆡᄀᆞ로ᄃᆡ밤이이무긥퍼거ᄂᆞᆯ네무슨긜거오미잇셔이러ᄒᆞᄂᆞ냐<end_of_turn>\n",
            "<start_of_turn>model\n",
            "세월이 여류하여 길동의 나이 팔세라. 상하 다 아니 칭찬할 이 없고 대감도 사랑하시나, 길동은 가슴의 원한이 부친을 부친이라 못하고 형을 형이라 부르지 못하매 스스로 천생됨을 자탄하더니, 칠월 망일에 명월을 대하여 정하에 배회하더니 추풍은 삽삽하고 기러기 우는 소리는 사람의 외로운 심사를 돕는지라.\n",
            "홀로 탄식하여 왈,\n",
            "\"대장부 세상에 나매 공맹의 도학을 배워 출장입상하여 대장인수를 요하에 차고 대장단에 높이 앉아 천병만마를 지휘중에 넣어두고, 남으로 초를 치고, 북으로 중원을 정하며, 서로 촉을 쳐 사업을 이룬 후에 얼굴을 기린각에 빛내고, 이름을 후세에 유전함이 대장부의 떳떳한 일이라. 옛 사람이 이르기를 '왕후장상이 씨없다.' 하였으니 나를 두고 이름인가. 세상 사람이 갈관박이라도 부형을 부형이라 하되 나는 홀로 그렇지 못하니 어떤 인생으로 그러한고.\"\n",
            "울억한 마음을 걷잡지 못하여 칼을 잡고 월하에 춤을 추며 장한 기운 이기지 못하더니, 이때 승상이 명월을 사랑하여 창을 열고 비겼더니, 길동의 거동을 보시고 놀래 가로되,\n",
            "\"밤이 이미 깊었거늘 네 무슨 즐거움이 있어 이러하느냐?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "길동이칼을던지고부복ᄃᆡ왈소인이ᄃᆡ감의졍긔을타당당ᄒᆞᆫ남ᄌᆞ로낫ᄉᆞ오니이만긜거ᄒᆞᆫ일이업ᄉᆞ오ᄃᆡ평셜위ᄒᆞ옵난아부를아부라부르지못ᄒᆞ옵고형을형이라못ᄒᆞ와상하노복이다쳔이보고친쳑고구도손으로가르쳐아모의쳔ᄉᆡᆼ이라이르오니이런원통ᄒᆞᆫ일이어ᄃᆡ잇ᄉᆞ오릿ᄀᆞ인ᄒᆞ여ᄃᆡ셩통곡ᄒᆞ니ᄃᆡ감이마음의긍측이녀긔시ᄂᆞ맛일그아음을위로ᄒᆞ면일노조ᄎᆞ방ᄌᆞᄒᆞᆯᄀᆞᄒᆞ야ᄭᅮ지져왈ᄌᆡ상의쳔ᄉᆡᆼ이너ᄲᅮᆫ아니라ᄀᆞ장방ᄌᆞᄒᆞᆫ마음을두지말나일후의다시그런말을번거이ᄒᆞᆫ일이잇스면눈압푸용납지못ᄒᆞ리라ᄒᆞ시니길동은한갓눈믈흘이ᄲᅮᆫ이라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "길동이 칼을 던지고 부복 대왈,\n",
            "\"소인은 대감의 정기를 타 당당한 남자로 낳사오니 이만 즐거운 일이 없사오되, 평(생) 설워하옵(기)는 아비를 아비라 부르지 못하옵고, 형을 형이라 못하여 상하 노복이 다 천히 보고, 친척 고두도 손으로 가르쳐 아무의 천생이라 이르오니 이런 원통한 일이 어디에 있사오리까?\" 인하여 대성통곡하니, 대감이 마음에 긍측이 여기시나 만일 그 마음을 위로하면 일로조차 방자할까 하여 꾸짖어 왈.\n",
            "\"재상의 천비 소생이 너 뿐 아니라. 자못 방자한 마음을 두지 말라. 일후에 다시 그런 말을 번거이 한 일이 있으면 눈 앞에 용납치 못하리라.\"\n",
            "하시니, 길동은 한갓 눈물 흘릴 뿐이라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "이윽키업듸엿더니ᄃᆡ감이믈너ᄀᆞ라ᄒᆞ시거날길동이도라와어미을붓들고통곡왈모친은소ᄌᆞ와젼ᄉᆡᆼ연분으로ᄎᆞᄉᆡᆼ의모ᄌᆞ되오니구뢰지은을ᄉᆡᆼ각ᄒᆞ오면호쳔망극ᄒᆞ오나남ᄋᆡ셰상의나셔입신양명ᄒᆞ와우희로향화을밧들고부모의약휵지은을만분의ᄒᆞᆫ나히라도갑푸거시여날이몸은팔ᄌᆞ긔박ᄒᆞ여쳔ᄉᆡᆼ이되여남의쳔ᄃᆡ을바드니ᄃᆡ장부엇지구구히근본을직히여후회을두리요이몸미당당히조션국병조판셔인슈을ᄯᅴ고상장군이되지못ᄒᆞᆯ진ᄃᆡᄎᆞ라리몸을산즁의븟쳐셰상영옥을모로고져ᄒᆞ오니복망모친은ᄌᆞ식의ᄉᆞ졍을ᄉᆞᆯ피ᄉᆞ아조바린다시잇고계시면후일의소ᄌᆞ도라와오조지졍을일위랄잇ᄉᆞ오니이만짐작ᄒᆞ옵소셔ᄒᆞ고언파의ᄉᆞ긔도도ᄒᆞ여도로혀비회업거날<end_of_turn>\n",
            "<start_of_turn>model\n",
            "이윽히 엎드려있더니, 대감이 물러가라 하시거늘, 길동이 돌아와 어미를 붙들고 통곡 왈,\n",
            "\"모친은 소자와 전생연분으로 차생에 모자 되오니 구로지은을 생각하오면 호천망극하오나, 남아가 세상에 나서 입신양명하여 위로 향화를 받들고, 부모의 양육지은을 만분의 하나라도 갚을 것이거늘, 이 몸은 팔자 기박하여 천생이 되어 남의 천대를 받으니, 대장부 어찌 구구히 근본을 지키어 후회를 두리요. 이 몸이 당당히 조선국 병조판서 인수를 띠고 상장군이 되지 못할진대, 차라리 몸을 산중에 붙여 세상영욕을 모르고자 하오니, 복망 모친은 자식의 사정을 살피사 아주 버린 듯이 잊고 계시면 후일에 소자 돌아와 오조지정을 이를 날 있사오니 이만 짐작하옵소서.\"\n",
            "하고, 언파에 사기 도도하여 도리어 비회 없거늘.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "그모이거동을보고ᄀᆡ유ᄒᆞ여왈ᄌᆡ상가쳔ᄉᆡᆼ이너ᄲᅮᆫ아니라무슨말을드른지모로되어미의간장을이ᄃᆡ지상케ᄒᆞᄂᆞᆫ다어미의낫츨보와아직잇스면ᄂᆡ두의ᄃᆡ감이쳐결ᄒᆞ시ᄂᆞᆫ분부업지아니ᄒᆞ리라길동이ᄀᆞ로ᄃᆡ부형의쳔ᄃᆡᄂᆞᆫ고ᄉᆞᄒᆞ옵고노복이며동유의잇다감들이ᄂᆞᆫ말이골슈의박키난일이허다ᄒᆞ오며근간의곡산모의ᄒᆡᆼᄉᆡᆨ을보오니승긔ᄌᆞ을염지ᄒᆞ야과실업ᄂᆞᆫ우리모ᄌᆞ을구슈갓치보와살ᄒᆡᄒᆡ할ᄯᅳ슬두오니불구의목젼ᄃᆡ환이잇슬지라그러ᄒᆞ오ᄂᆞ소ᄌᆞ나ᄀᆞᆫ후이라도모친의게환이밋지아니케ᄒᆞ오리다그어미ᄀᆞ로ᄃᆡ네말이ᄀᆞ장그러ᄒᆞᄂᆞ곡ᄉᆞᆫ모ᄂᆞᆫ인후ᄒᆞᆫᄉᆞᄅᆞᆷ이라엇지그런일이잇스리요길동왈셰상ᄉᆞ을층양치못ᄒᆞᄂᆞ이다소ᄌᆞ의말을헛도히ᄉᆡᆼ각지마르시고쟝ᄂᆡ을보오쇼셔ᄒᆞ더라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "그 모 이 거동을 보고 개유하여 왈,\n",
            "\"재상가 천생이 너뿐 아니라. 무슨 말을 들었는지 모르되 어미의 간장을 이다지 상케 하느냐? 어미의 낮을 보아 아직 있으면 내두에 대감이 처결하시는 분부 없지 아니하리라.\"\n",
            "길동이 가로되,\n",
            "\"부형의 천대는 고사하옵고, 노복이며 동유의 이따금 들리는 말이 골수에 박히는 일이 허다하오며, 근간에 곡산모의 행색을 보오니 승기자를 염지하여 과실없는 우리 모자를 구수같이 보아 살해 해할 뜻을 두오니 불구에 목전대환이 있을지라. 그러하오나 소자 나간 후 이라도 모친에게 후환이 미치지 아니케 하오리다.\"\n",
            "그 어미 가로되.\n",
            "\"네 말이 자못 그러하나 곡산모는 인후한 사람이라. 어찌 그런 일이 있으리요?\"\n",
            "길동 왈,\n",
            "\"세상사를 측량치 못하나이다. 소자의 말을 헛되이 생각지 마시고 장래를 보읍소서.\" 하더라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "원ᄂᆡ곡산모는곡산긔ᄉᆡᆼ으로ᄃᆡ감의총쳡이되여ᄯᅳ시방ᄌᆞᄒᆞ긔로노복이라도블합ᄒᆞᆫ일이잇스면ᄒᆞᆫ번참소의ᄉᆞᄉᆡᆼ이관계ᄒᆞ여ᄉᆞᄅᆞᆷ이못되면긧거ᄒᆞ고승ᄒᆞ면시긔ᄒᆞ더니ᄃᆡ감이용몽을엇고길동을나허ᄉᆞᄅᆞᆷ마닥일칼고ᄃᆡ감이ᄉᆞ랑ᄒᆞ시ᄆᆡ일후총을아일ᄀᆞᄒᆞ며ᄯᅩᄒᆞᆫᄃᆡ감이잇다감희롱ᄒᆞ시난말ᄉᆞᆷ이너도길동갓탄ᄌᆞ식을나허ᄂᆡ의모년ᄌᆞ미을도으라ᄒᆞ시ᄆᆡᄀᆞ쟝무류ᄒᆞ여ᄒᆞᄂᆞᆫ즁의길동의일홈미날노ᄌᆞᄌᆞᄒᆞ무로초낭더옥크게시긔ᄒᆞ여길동모ᄌᆞ을눈의ᄀᆞ시ᄀᆞ치미워ᄒᆞ여ᄒᆡ할마음이급ᄒᆞᄆᆡ흉계을ᄌᆞ어ᄂᆡ여ᄌᆡ물을흣터요괴로온무녀등을블너모의말말ᄒᆞ고츅일왕ᄂᆡᄒᆞ더니ᄒᆞᆫ무녀ᄀᆞ로ᄃᆡ동ᄃᆡ문밧긔관상ᄒᆞ난계집이잇스되ᄉᆞᄅᆞᆷ의상을ᄒᆞᆫ번보오면평ᄉᆡᆼ길흉화복을판단ᄒᆞ오니이졔쳥ᄒᆞ여약속을졍ᄒᆞ고ᄃᆡ감젼의쳔거ᄒᆞ여ᄀᆞ즁젼후ᄉᆞ을본다시이른후의인ᄒᆞ여길동의상을보고어ᄎᆞ어ᄎᆞ이알외여ᄃᆡ감의마음을놀ᄂᆡ면낭ᄌᆞ의소회를일노조ᄎᆞ일울ᄀᆞᄒᆞᄂᆞ이다초낭이ᄃᆡ희ᄒᆞ야직시관상녀의게통ᄒᆞ여ᄌᆡ믈노ᄡᅧ다ᄅᆡ고ᄃᆡ감ᄃᆡᆨ일을낫낫치ᄀᆞ르치고길동졔거ᄒᆞᆯ약속을졍ᄒᆞᆫ후의날을긔약ᄒᆞ고보ᄂᆡ니라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "원래 곡산모는 곡산 기생으로 대감의 총첩이 되어 뜻이 방자하기로, 노복이라도 불합한 일이 있으면 한 번 참소에 사생이 관계하여 사람이 못되면 기뻐하고 승하면 시기하더니, 대감이 용몽을 얻고 길동을 낳아 사람마다 일컫고 대감이 사랑하시매, 일후 총을 앗길까 하며, 또한 대감이 이따금 희롱하시는 말씀이\n",
            "\"너도 길동같은 자식을 낳아 나의 모년재미를 도우라.\"\n",
            "하시매, 가장 무료하여 하는 중에 길동의 이름이 날로 자자하므로 초낭 더욱 크게 시기하여 길동 모자를 눈의 가시같이 미워하여 해할 마음이 급하매, 흉계를 짜아내어 재물을 흩어 요괴로운 무녀 등을 불러 모의말 말하고 축일왕래하더니, 한 무녀 가로되,\n",
            "\"동대문 밖에 관상하는 계집이 있으되, 사랑의 상을 한 번 보면 평생 길흉화복을 판단하오니, 이제 청하여 약속을 정하고 대감전에 천거하여 가중 전후사를 본 듯이 이른 후에 인하여 길동의 상을 보고 여차여차히 아뢰어 대감의 마음을 놀래면 낭자의 소회를 이룰까 하나이다.\"\n",
            "초낭이 대희하여, 즉시 관상녀에계 통하여 재물로써 달래고, 대감댁 일을 낱낱이 가르치고, 길동 제거할 약속을 정한 후에 날을 기약하고 보내니라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "일일은ᄃᆡ감이ᄂᆡ당의드러ᄀᆞ길동을불은후의부인을ᄃᆡᄒᆞ야ᄀᆞ로ᄃᆡ이아희비록영웅의긔상이잇스나엇다쓰리요ᄒᆞ시며히롱ᄒᆞ시더니믄득ᄒᆞᆫ녀ᄌᆞ밧긔로븟터드러와당하의뵈거날ᄃᆡ감이괴히녀겨그연고을무르신ᄃᆡ그녀ᄌᆞ복지쥬왈소녀난동ᄃᆡ문밧긔ᄉᆞ옵더니어려셔ᄒᆞᆫ도인을만ᄂᆞᄉᆞᄅᆞᆷ의상보는볍을ᄇᆡ은바두로다니며관상ᄎᆞ로맛호장안을편남ᄒᆞ옵고ᄃᆡ감ᄃᆡᆨ만복을놉피듯고쳔ᄒᆞᆫᄌᆡ조을시험코져왓ᄂᆞ니다ᄃᆡ감이엇지요괴로은무녀을ᄃᆡᄒᆞ여문답이잇스리요마ᄂᆞᆫ길동을히롱ᄒᆞ시던ᄭᅳᆺ친고로우으시며왈네암커ᄂᆞ갓ᄀᆞ히올아ᄂᆡ의평ᄉᆡᆼ을확논ᄒᆞ라ᄒᆞ시니관상녀국궁ᄒᆞ고당의올나몬쳠ᄃᆡ감의상을ᄉᆞᆯ핀후의이왕지ᄉᆞ을역역히알외며ᄂᆡ두ᄉᆞ을보ᄂᆞᆫ다시논단ᄒᆞ니호발도ᄃᆡ감의마음의위월ᄒᆞᆫ마듸업ᄂᆞᆫ지라ᄃᆡ감이크게층찬ᄒᆞ시고연ᄒᆞ여ᄀᆞ즁ᄉᆞᄅᆞᆷ의상을의논할ᄉᆡ낫낫치본다시폄논ᄒᆞ야ᄒᆞᆫ말도허망ᄒᆞᆫ고시업ᄂᆞᆫ지라ᄃᆡ감과부인이며좌즁졔인이ᄃᆡ혹ᄒᆞ야신인이라일ᄏᆞᆺ더라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "일일은 대감이 내당에 들어가 길동을 부른 후에 부인을 대하여 가로되,\n",
            "\"이 아이 비록 영웅의 기상이 있으나 어디다 쓰리요.\"\n",
            "하시며 희롱하시더니, 문득 한 여자 밖으로부터 들어와 당하에 뵈거늘, 대감이 괴히 여겨 그 연고를 물으신대, 그 여자 복지 주왈.\n",
            "\"소녀는 동대문 밖에 사옵더니, 어려서 한 도인을 만나 사람의 상보는 법을 배운 바 두루 다니며 관상차로 만호장안을 편람하옵고, 대감댁 만복을 높이 듣고 천한 재주를 시험코자 왔나이다.\"\n",
            "대감이 어찌 요괴로운 무녀를 대하여 문답이 있으리요마는 길동을 희롱하시던 끝인 고로 웃으시며 왈,\n",
            "\"네 암커나 가까이 올라 나의 평생을 확론하라.\"\n",
            "하시니, 관상녀 국궁하고 당에 올라 먼저 대감의 상을 살핀 후에 이왕지사를 역역히 아뢰며 내두사를 보는 듯이 논단하니, 호발도 대감의 마음에 위월한 마디 없는지라. 대감이 크게 칭찬하시고 연하여 가중 사람의 상을 의논할새, 낱낱이 본 듯이 평론하여 한 말도 허망한 곳이 없는지라. 대감과 부인이며 좌중제인이 대혹하여 신인이라 일컫터라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "ᄭᅳᆺ틔로길동의상을의논ᄒᆞᆯᄉᆡ크게층찬왈소녀ᄀᆞ열읍의쥬류ᄒᆞ며쳔만인을보와시되공ᄌᆞ의상갓튼이ᄂᆞᆫ쳐음이연이와아지못게라부인의긔츌이아니ᄀᆞᄒᆞᄂᆞ이다ᄃᆡ감이쇼긔지못ᄒᆞ여왈그는그러ᄒᆞ거니와ᄉᆞᄅᆞᆷ마닷길흉영욕이각각ᄯᆡ잇ᄂᆞᆫ이이아희상을각별논단ᄒᆞ라ᄒᆞ니상녜이윽키보다ᄀᆞ거즛놀ᄂᆡᄂᆞᆫ쳬ᄒᆞ거날괴히녀겨그연고을므르신ᄃᆡ함구ᄒᆞ고말이업거날ᄃᆡ감이ᄀᆞ로ᄃᆡ길흉을호발도긔이지말고보이ᄂᆞᆫᄃᆡ로의논ᄒᆞ여ᄂᆡ의의혹이업게ᄒᆞ라관상녀ᄀᆞ로ᄃᆡ이말ᄉᆞᆷ을바로알외오면ᄃᆡ감의마음을놀ᄂᆡᆯᄀᆞᄒᆞᄂᆞ이다ᄃᆡ감왈옛졔곽분양ᄀᆞᆺᄐᆞᆫᄉᆞᄅᆞᆷ도길ᄒᆞᆫᄯᆡ잇고흉ᄒᆞᆫᄯᆡ잇셔시니무슨여러말이잇난요상볍보이ᄂᆞᆫᄃᆡ로긔이말나ᄒᆞ이니관상녀마지못ᄒᆞ여길동치운후의그윽키알외되공ᄌᆞ의ᄂᆡ두ᄉᆞᄂᆞᆫ여러말ᄉᆞᆷ발이옵고셩즉군왕지상이요ᄑᆡ즉층양치못ᄒᆞᆯ환이잇ᄂᆞᆫ이다ᄒᆞᆫᄃᆡᄃᆡ감이크게놀ᄂᆡ여윽키진졍ᄒᆞᆫ후의상녀를후이상급ᄒᆞ시고ᄀᆞ로ᄃᆡ이ᄃᆡ여말을ᄉᆞᆷᄀᆞ발구치말나엠이분부ᄒᆞ시고왈졔늑드락츄립지못ᄒᆞ게ᄒᆞ리라ᄒᆞ시니상녀왈왕후장상이엇지씨잇스릿ᄀᆞᄃᆡ감이누누당부ᄒᆞ시니관상녀공슈슈명ᄒᆞ고가니라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "끝으로 길동의 상을 의논할새, 크게 칭찬 왈,\n",
            "\"소녀가 열읍에 주류하며 천만인을 보았으되 공자의 상같은 이는 처음이려니와 알지 못게라, 부인의 기출이 아닌가 하나이다.\"\n",
            "대감이 속이지 못하여 왈,\n",
            "\"그는 그러하거니와 사람마다 길흥영욕이 각각 때있나니 이 아이 상을 각별 논단하라.\"\n",
            "하니, 상녀가 이윽히 보다가 거짓 놀라는 체 하거늘, 괴히 여겨 그연고를 물으신대 함구하고 말이 없거늘, 대감이 가로되,\n",
            "\"길흉을 호발도 기이지 말고 보이는 대로 의논하여 나의 의흑이 없게 하라.\"\n",
            "관상녀 가로되,\n",
            "\"이 말씀을 바로 아뢰면 대감의 마음을 놀래일까 하나이다.\"\n",
            "대감 왈,\n",
            "\"옛날 곽분양같은 사람도 길한 때 있고 흉한 때있었으니 무슨 여러 말이 있느냐? 상법 보이는 대로 기이 말라.\"\n",
            "하시니. 관상터 마지 못하혀 길동을 치운 후에 그윽히 아뢰되,\n",
            "\"공자의 내두사는 여러 말씀 버리옵고 성즉 군왕지상이요, 패즉 측량치 못할 환이 있나이다.\"\n",
            "한대, 대감이 크계 놀래어 (이)윽히 진정란 후에 상녀를 후이 상급하시고 가로되,\n",
            "\"이같은 말을 삼가 발구치 말라.\"\n",
            "엄히 분부하시고, 왈,\n",
            "\"제 늙도록 출입치 못하게 하리라.\"\n",
            "하시니, 상녀 왈,\n",
            "\"왕후장상이 어디 씨 있으리까?\"\n",
            "대감이 누누당부하시니, 관상녀 공수 수명하고 가니라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "ᄃᆡ감이발을드르신후로ᄂᆡ렴의크게근심ᄒᆞᄉᆞ일염의ᄉᆡᆼ각ᄒᆞ시되이놈이본ᄅᆡ범상ᄒᆞᆫ놈이아니요ᄯᅩᄒᆞᆫ쳔ᄉᆡᆼ되물ᄌᆞᄐᆞᆫᄒᆞ여만일범남ᄒᆞᆫ마음을머그면누ᄃᆡ갈츙보국ᄒᆞ던일이쓸ᄃᆡ업고ᄃᆡ화일문의밋츠리니밀이져을업셰여ᄀᆞ화을덜고져ᄒᆞᄂᆞ인졍의ᄎᆞ마못ᄒᆞᆯᄇᆡ라ᄉᆡᆼ각이이려ᄒᆞᆫ즉션쳐ᄒᆞᆯ도리업셔일념이병이되여식불감침불안ᄒᆞ시ᄂᆞᆫ지라초낭이긔ᄉᆡᆨ을ᄉᆞᆯ핀후의승간ᄒᆞ여엿ᄌᆞ오ᄃᆡ길동이관상여의말ᄉᆞᆷ갓치왕긔잇셔만일범남ᄒᆞᆫ일이잇ᄉᆞ오면ᄀᆞ화장ᄎᆞ측냥치못ᄒᆞᆯ지라어린소견은져근혐의를ᄉᆡᆼ각지말으시고큰일을ᄉᆡᆼ각ᄒᆞ와져를미리업시ᄒᆞᆷ만갓지못ᄒᆞᆯᄀᆞᄒᆞᄂᆞ이다<end_of_turn>\n",
            "<start_of_turn>model\n",
            "대감이 이 말을 들으신 후로 내념에 크게 근심하사 일념에 생각하시되,\n",
            "\"이놈이 본래 범상한 놈이 아니요, 또 천생됨을 자탄하여 만일 범람한 마음을 먹으면 누대 갈충보국하던 일이 쓸데없고 대화 일문에 미치리니 미리 저를 없애어 가화를 덜고자 하나 인정에 차마 못할 바라.\"\n",
            "생각이 이러한즉 선처할 도리없어 일념이 병이 되어 식불감 침불안하시는지라.\n",
            "초낭이 기색을 살핀 후에 승간하여 여쭈오되,\n",
            "\"길동이 관상년의 말씀같이 왕기 있어 만일 범람한 일이 있사오면 가화 장차 측량치 못할지라. 어리석은 소견은 적은 혐의를 생각지 마시고 큰 일을 생각하여 저를 미리 없이 함만 같지 못할까 하나이다.\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "ᄃᆡ감이ᄃᆡᄎᆡᆨ왈이말을경솔이ᄒᆞᆯᄇᆡ아니여날네엇지입을직키지못ᄒᆞᄂᆞ뇨도시ᄂᆡ집ᄀᆞ운을네알ᄇᆡ아니이라ᄒᆞ시니초낭이황공ᄒᆞ여다시말ᄉᆞᆷ을못ᄒᆞ고ᄂᆡ당의드러ᄀᆞ부인과ᄃᆡ감의장ᄌᆞ을ᄃᆡᄒᆞ야엿ᄌᆞ오되ᄃᆡ감이관상녀의말ᄉᆞᆷ을드르신후로ᄉᆞ렴의션쳐ᄒᆞ실도리업ᄉᆞ와침식이불안ᄒᆞ시더니일렴의병환이되시긔로소인이일젼의여ᄎᆞ여ᄎᆞᄒᆞᆫ말ᄉᆞᆷ을알외온즉ᄭᅮ종이낫삽긔로다시엿ᄌᆞᆸ지못ᄒᆞ여ᄊᆞᆸ거니와소인이ᄃᆡ감의마음을취ᄐᆡᆨᄒᆞ온즉ᄃᆡ감계옵셔도져를미리업셰고져ᄒᆞ시되ᄒᆞ마거쳐치못ᄒᆞ오니미련ᄒᆞᆫ소견으로ᄂᆞᆫ션쳐ᄒᆞᆯ모ᄎᆡᆨ이길동을몬져업신후의ᄃᆡ감ᄭᅴ아뢰즉이위져즌일이라ᄃᆡ감계옵셔도엇지할슈업ᄉᆞ와마ᄋᆞᆷ을아조이즐ᄀᆞᄒᆞ옵ᄂᆞ이다<end_of_turn>\n",
            "<start_of_turn>model\n",
            "대감이 대책 왈,\n",
            "\"이 말을 경솥히 할 바가 아니거늘, 네 어쩌 입을 지키지 뭇하느냐? 도시 내 집 가운을 네 알 바가 아니라.\"\n",
            "하시니, 초낭이 황공하여 다시 말씀을 못하고, 내당에 들어아 부인과 대감의 장자를 대하여 여쭈오되,\n",
            "\"대감이 관상녀의 말씀을 들으신 후로 사념에 선처하실 도리 없사와 침식이 불안하시더니 일념의 병환이 되시기로 소인이 일전에 여차여차한 말씀을 아뢰온즉 꾸중이 났는 고로 다시 여쭙지 못하였거니와, 소인이 대감의 마음을 취택하온즉 대감께서도 저를 미리 없애고자 하시되 차바 거처치 못하오니, 미련한 소견으로는 선처할 모책이 길동을 먼저 없앤 후에 대감께 아뢰면 이미 저질러진 일이라 대감께서도 어찌 할 수 업사와 마음을 아주 잊을까 하옵나이다.\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "부인이빈츅왈일은그러ᄒᆞ거니와인졍쳔리의ᄎᆞ마ᄒᆞᆯᄇᆡ안이라ᄒᆞ시니초낭이다시엿ᄌᆞ오ᄃᆡ이일이여러ᄀᆞ지관겨ᄒᆞ오니ᄒᆞᆫ나흔국ᄀᆞ을위ᄒᆞᆷ미요두른은ᄃᆡ감의환후을위ᄒᆞ미요셰슨홍씨일문을위ᄒᆞ미요니엇지져근ᄉᆞ졍으로우유부단ᄒᆞ와여러ᄀᆞ지큰일을ᄉᆡᆼ각지아니ᄒᆞ시다ᄀᆞ후회막급이되오면엇지ᄒᆞ오릿ᄀᆞᄒᆞ며만단으로부인과ᄃᆡ감의장ᄌᆞ을달ᄂᆡ니마지못ᄒᆞ여허락ᄒᆞ시거날초낭이암희ᄒᆞ야나와특ᄌᆞ라ᄒᆞᄂᆞᆫᄌᆞᄀᆡᆨ을쳥ᄒᆞ여슈말을다젼ᄒᆞ고은ᄌᆞ을만이쥬워오날밤의길동을ᄒᆡᄒᆞ라약속을졍ᄒᆞ고다시ᄂᆡ당의드러ᄀᆞ부인젼의슈말을엿ᄌᆞ오니부인이드르시고발을구르시며못ᄂᆡᄎᆞ셕ᄒᆞ시더라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "부인이 빈축 왈,\n",
            "\"일은 그러하거니와 인정천리에 차마 할 바가 아니라.\"\n",
            "하시니, 초낭이 다시 어쭈오되,\n",
            "\"이 일이 여러가지 관계하오니, 하나는 국가를 위함이요, 둘은 대감의 환후를 위함이요, 셋은 홍씨 일문을 위함이오니, 어찌 적은 사정으로 우유부단하여 여러가지 큰 일을 생각지 아니하시다가 후회막급이 되오면 어리 하오리까?\"\n",
            "하며, 만단으로 부인과 대감의 장자를 달래니, 마지 못하여 허락하시거늘, 초낭이 암회하여 나와 특자라 하는 자객을 청하여 수말을 다 전하고 은자를 많이 주어 오늘 밤에 길동을 해하라 약속을 정하고, 다시 내당에 들어가 부인전에 수말을 여쭈오니, 부인이 들으시고 발을 구르시며 못내 차석하시더라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "이젹의길동은나희십일셰라기골이쟝ᄃᆡᄒᆞ고용ᄆᆡᆼ이졀뉸ᄒᆞ며시셔ᄇᆡᆨᄀᆞ여을무블통지ᄒᆞᄂᆞᄃᆡ감분부의밧긔츌입을막으시ᄆᆡ홀노별당의쳐ᄒᆞ여손오의병셔을통니ᄒᆞ여귀신도측냥치못ᄒᆞᄂᆞᆫ슐볍이며쳔지조화을품어풍운을임의로부리며육졍육갑의신장을부려신츌귀몰지술을통달ᄒᆞ니셰상의두려온거시업더라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "이때의 길동은 나이 십일세라. 기골이 장대하고, 총맹이 절륜하며, 시서백가어를 무불통지하나, 대감 분부에 바깥 출입을 막으시매, 홀로 별당에 처하여 손오의 병서를 통리하여 귀신도 즉량치 못하는 술법이며 천지조화를 품어 풍운을 임의로 부리며, 육정육갑이 신장을 부려 신출귀몰지술을 통달하니 세상에 두려운 것이 없더라.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "이날밤ᄉᆞᆷ경이된후의쟝ᄎᆞ셔안을물이치고취침ᄒᆞ려ᄒᆞ더니문득창밧긔셔ᄀᆞ마귀셰변울고셔으로나라ᄀᆞ거날마ᄋᆞᆷ의놀ᄂᆡᄒᆡ혹ᄒᆞ니ᄀᆞ마귀셰변ᄀᆡᆨᄌᆞ와ᄀᆡᆨᄌᆞ와ᄒᆞ고셔으로나라ᄀᆞ나분병ᄌᆞᄀᆡᆨ이오는지라엇던ᄉᆞᄅᆞᆷ이날을ᄒᆡ코져ᄒᆞᄂᆞᆫ고암커ᄂᆞ방신지게을ᄒᆞ리라ᄒᆞ고방즁의팔진을치고각각방위을밧고와남방의이허즁운북방의감즁연의옴긔고동방진하연은셔방ᄐᆡ상졀의옴긔고건방의건ᄉᆞᆷ연은숀방손하졀의옴긔고곤방의곤ᄉᆞᆷ졀은간방간상연의옴겨그ᄀᆞ온ᄃᆡ풍운을너허조화무궁케버리고ᄯᆡ을긔다리니라<end_of_turn>\n",
            "<start_of_turn>model\n",
            "이날 밤 삼경이 된 후에 장차 서안을 물리치고 취침하려 하더니 문득 창 밖에서 까마귀 세 번 울고 서로 날아가거늘, 마음에 놀래 해혹하니,\n",
            "\"까마귀 세 번 '객자와 객자와' 하고 서로 날아가니 분명 자객이 오는지라. 어떤 사람이 나를 해코자 하는고? 암커나 방신지계를 하니라.\"\n",
            "하고, 방중에 팔진을 치고 각각 방위를 바꾸어, 남방의 이허중은 북방의 감중련에 옮기고, 동방 진하련은 서방 태상절에 옮기고, 건방의 건삼련은 손방 손하절에 옮기고, 곤방의 곤삼절은 간방 간상련에 옮겨, 그 가운데 풍운을 넣어 조화무궁페 벌리고 때를 기다리니다.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "이젹의특ᄌᆞ비슈을들고길동거쳐ᄒᆞᄂᆞᆫ병당의ᄀᆞ몸을슘긔고그ᄌᆞᆷ들긔을긔다리더니난ᄃᆡ업슨ᄀᆞ마귀창밧긔와울고ᄀᆞ거날마음의크게의심ᄒᆞ여왈이김ᄉᆡᆼ이무슨알미잇셔쳔긔을누셜ᄒᆞᄂᆞᆫ고길동은실노범상ᄒᆞᆫᄉᆞᄅᆞᆷ이아니로다피련다일의크게쓰리라ᄒᆞ고도라ᄀᆞ고져ᄒᆞ다ᄀᆞ은ᄌᆡ의욕심이몸을ᄉᆡᆼ각지못ᄒᆞ야이시ᄒᆞᆫ후몸을날여방즁의드러ᄀᆞ니길동은간ᄃᆡ업고일진광풍이이러나뇌셩벽녁이쳔지진동ᄒᆞ며운무ᄌᆞ옥ᄒᆞ여동셔을분별치못ᄒᆞ며좌우을살펴보니쳔봉만학이즁즁쳡쳡ᄒᆞ고ᄃᆡᄒᆡ창일ᄒᆞ야졍신을슈십지못ᄒᆞᄂᆞᆫ지라특ᄌᆞᄂᆡ렴의혜아리되ᄂᆡ앗가분명방즁의드러와거든산은어인산이며물은어인물인고ᄒᆞ야갈바을아지못ᄒᆞ더니문득옥져소ᄅᆡ드리거날살펴보니쳥의동ᄌᆞᄇᆡᆨ학을타고공즁의다니며불너왈너ᄂᆞᆫ엇더ᄒᆞᆫᄉᆞᄅᆞᆷ이과ᄃᆡ이집푼밤의비슈을들고뉘를ᄒᆡ코져ᄒᆞᄂᆞᆫ다특ᄌᆞᄃᆡ왈네분명길동이로다나ᄂᆞᆫ너희부형의명영을바다너를ᄎᆔᄒᆞ려왓노라ᄒᆞ고비슈을드러더지니문득길동은간ᄃᆡ업고음풍이ᄃᆡ작ᄒᆞ고벽녁이진동ᄒᆞ며즁쳔의살긔ᄲᅮᆫ이로다<end_of_turn>\n",
            "<start_of_turn>model\n",
            "이때에 특자 비수를 들고 길동 거처하는 별당에 가서 몸을 숨기고 그 잠들기를 기다리더니, 난데없는 까마귀 창 밖에 와 울고 가거늘 마음에 크게 의심하여 왈,\n",
            "\"이 짐승이 무슨 앎이 있어 천기를 누설하는고? 길동은 실로 범상한 사람이 아니로다. 필연 타일에 크게 쓰리라.\"\n",
            "하고, 돌아가고자 하다가 은자에의 욕심이 몸을 생각치 못하여 이시한후에 몸을 날려 방중에 들어가니, 길동은 간 데 억고, 일진광풍이 일어나 뇌성벽력이 천지 진동하며 운무 자욱하여 동서를 분별치 못하며 좌우를 살펴보니 천봉만학이 중중첩첩하고, 대해 창일하여 정신을 수습치 못하는지라.\n",
            "특자 내념에 헤아리되,\n",
            "\"내 아까 분명 방중에 들어왔거늘 산은 어인 산이며, 물은 어인 물인고?\"\n",
            "하여 갈 바를 알지 못하더니, 문득 옥적소리 들리거늘, 살펴보니 청의동자 백학을 타고 공중에 다니며 불러 왈,\n",
            "\"너는 어떠한 사람이관대 이 깊은 밤에 비수를 들고 누구를 해코자 하느냐?\"\n",
            "특자 대왈,\n",
            "\"네 분명 길동이로다. 나는 너의 부형의 명령을 받아 너를 취하러 왔노라.\"\n",
            "하고 비수를 들어 던지니, 문득 길동은 간 데 없고, 음풍이 대작하고 벽력이 진동하며, 중천에 살기 뿐이로다.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "즁심의ᄃᆡ겁ᄒᆞ여칼을ᄎᆞ즈며왈ᄂᆡ남의ᄌᆡ물을욕심ᄒᆞ다ᄀᆞᄉᆞ지예ᄲᆞ졋쓰니슈원슈구ᄒᆞ리요ᄒᆞ며긔리탄식ᄒᆞ더니문득이윽고길동이비슈을들고공즁의셔위여왈필부ᄂᆞᆫ드르라네ᄌᆡ물을탐ᄒᆞ여무죄ᄒᆞᆫ인명을살ᄒᆡ코져ᄒᆞ니이졔너을살녀두멘일후의무죄ᄒᆞᆫᄉᆞᄅᆞᆷ이허다이상얼지라엇지살녀보ᄂᆡ리요ᄒᆞᆫᄃᆡ특ᄌᆞᄋᆡ결왈과연소인의죄아니오라공ᄌᆞᆺᄃᆡᆨ초낭ᄌᆞ의소위오니바ᄅᆡ옵건ᄃᆡᄀᆞ련ᄒᆞᆫ인명을구졔ᄒᆞ옵셔일후의ᄀᆡ과ᄒᆞ게ᄒᆞ옵소셔길동이더옥분을이긔지못ᄒᆞ야왈네의약관이하날의ᄉᆞ못ᄎᆞ오날날ᄂᆡ손을비러악ᄒᆞᆫ유을업시게ᄒᆞ미라ᄒᆞ고언파의특ᄌᆞ의목을쳐바리고신장을호령ᄒᆞ여동ᄃᆡ문밧긔상녀을ᄌᆞᄇᆞᄃᆞᄀᆞ수죄ᄒᆞ여왈네요망ᄒᆞᆫ년으로ᄌᆡ상ᄀᆞ의출입ᄒᆞ며인명을상ᄒᆡᄒᆞ니네죄을네아ᄂᆞᆫ다<end_of_turn>\n",
            "<start_of_turn>model\n",
            "중심에 대겁하여 칼을 찾으며 왈,\n",
            "\"내 남의 재물을 욕심하다가 사지에 빠졌으니 수원수구하리요.\"\n",
            "하며, 길게 탄식하더니, 문득 이윽고 길동이 비수를 들고 공중에서 외쳐 왈,\n",
            "\"필부는 들으라. 네 재물을 탐하여 무죄한 인명을 살해코자 하니 이제 너를 살려두면 일후에 무죄한 사람이 허다히 상할지라. 어찌 살려 보내리요.\"\n",
            "한대, 특자 애걸 왈,\n",
            "\"과연 소인의 죄 아니오라 공잣댁 초낭자의 소위오니, 바라옵건데 가련한 인명을 구제하셔서 일후에 개과하게 하옵소서.\"\n",
            "길동이 더욱 분을 이기지 못하여 왈,\n",
            "\"너의 약관이 하늘에 사무쳐 오늘날 나의 손을 빌어 악한 유를 없애게 함이라.\"\n",
            "하고, 언파에 특자의 목을 쳐버리고, 신장을 호령하여 동대문 밖의 상녀를 잡아다가 수죄하여 왈,\n",
            "\"네 요망한 년으로 재상가에 출입하며 인명을 상해하니 네 죄를 네 아느냐?\"\n",
            "<end_of_turn>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"bebechien/HongGildongJeon\",\n",
        "    split=\"train\",\n",
        ")\n",
        "print(ds)\n",
        "data = ds.with_format(\n",
        "    \"np\", columns=[\"original\", \"modern translation\"], output_all_columns=False\n",
        ")\n",
        "\n",
        "in_context_learning = \"\"\n",
        "in_context_length = 0\n",
        "\n",
        "in_context_limit = CACHE_LENGTH - MAX_NEW_TOKENS\n",
        "\n",
        "for x in data:\n",
        "    item = f\"<start_of_turn>user\\n{x['original']}<end_of_turn>\\n<start_of_turn>model\\n{x['modern translation']}<end_of_turn>\\n\"\n",
        "    length = len(tokenizer.encode(item))\n",
        "    if in_context_length + length < in_context_limit:\n",
        "        in_context_length += length\n",
        "        in_context_learning += item\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(in_context_length)\n",
        "print(in_context_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjunN0TqFrbs"
      },
      "source": [
        "## Generate Output\n",
        "\n",
        "Define helper function and test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjejSEnC2hbG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "tick_start = 0\n",
        "\n",
        "def tick():\n",
        "    global tick_start\n",
        "    tick_start = time.time()\n",
        "\n",
        "def tock():\n",
        "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
        "\n",
        "test_cases = [\n",
        "    \"ᄃᆡ작ᄒᆞ여그ᄭᅩᆺ치흣터지거ᄂᆞᆯ\",\n",
        "    \"금두겁이품의드러뵈니일졍ᄌᆡᄌᆞᄅᆞᆯ나흐리로다ᄒᆞ더니과연그ᄃᆞᆯ부터잉ᄐᆡᄒᆞ여십삭이차니\",\n",
        "    \"이ᄯᆡᄂᆞᆫᄉᆞ월초팔일이라이날밤의오ᄉᆡᆨ구룸이집을두루고향ᄂᆡ진동ᄒᆞ며션녀ᄒᆞᆫᄡᅣᆼ이촉을들고드러와김ᄉᆡᆼᄃᆞ려니르ᄃᆡ\",\n",
        "    \"ᄌᆡ히길너텬졍을어긔지말으소셔이아희ᄇᆡ필은낙양니샹셔집아ᄌᆡ니\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMabwOWLu-wm"
      },
      "source": [
        "### Without In-context Learning\n",
        "\n",
        "The output below shows that the model doesn't recognize word or phrase and has no capability to translate Early Hangul."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tEIyBr0vH81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "క్షమించండి, నేను ఆ ప్రశ్నకు సమాధానం ఇవ్వలేను. నేను హానికరమైన లేదా ప్రమాదకరమైన ప్రతిస్పందనలను ఉత్పత్తి చేయడానికి రూపొందించబడలేదు.<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 29.95s\n",
            "--------------------------------------------------------------------------------\n",
            "이 문장은 아름다운 한국어 시조입니다. \n",
            "\n",
            "**번역:**\n",
            "\n",
            "금두겁이 품에 안겨 일정한 자리에서,\n",
            "드러뵈니, 놀라움이 일었습니다.\n",
            "과연 그 드러내니, 낯선 곳에서 시작되어,\n",
            "다시 다녀보니, 사삭이 차니.\n",
            "\n",
            "**해석:**\n",
            "\n",
            "금두겁이 덮여 있는 자리에서,\n",
            "어떤 놀라운 일이 일어났는지,\n",
            "그 드러내니, 낯선 곳에서 시작되어,\n",
            "다시 돌아와보니, 사삭이 차니.\n",
            "\n",
            "**추가 설명:**\n",
            "\n",
            "*   **금두겁이:** 금을 덮여 있는, 덮여진 상태를 의미합니다.\n",
            "*   **일졍ᄌᆡᄌᆞ:** (일정) 낯선, 낯선 장소\n",
            "*   **ᄅᆞᆯ나흐리로다ᄒᆞ더니:** (드러뵈니) 드러나다, 튀어나다\n",
            "*   **과연그ᄃᆞᆯ부터잉:** (그 드러내니) 그 모습, 그 모습이\n",
            "*   **ᄐᆡᄒᆞ더니:** (다녀보니)\n",
            "TOTAL TIME ELAPSED: 23.34s\n",
            "--------------------------------------------------------------------------------\n",
            "이 문장은 매우 어색하고, 의미를 파악하기 어렵습니다. \n",
            "\n",
            "다만, 몇 가지 추측을 해볼 수 있습니다.\n",
            "\n",
            "* **문맥:** 이 문장은 아마도 특정 상황이나 이야기를 암시하는 것 같습니다.\n",
            "* **어색한 표현:** \"이ᄯᆡᄂᆞᆫᄉᆞ월초팔일이라이날밤의오ᄉᆡᆨ구룸이집을두루고향ᄂᆡ진동ᄒᆞ며션녀ᄒᆞᆫᄡᅣᆼ이촉을들고드러와김ᄉᆡᆼᄃᆞ려니르\" 와 같은 표현은 문법적으로 어색하고 의미가 불분명합니다.\n",
            "* **가능성:** 이 문장은 아마도 특정 장소나 상황에 대한 묘사일 수 있습니다. 예를 들어, 특정 지역의 풍경이나 분위기를 묘사하는 것일 수도 있습니다.\n",
            "\n",
            "더 정확한 분석을 위해서는 이 문장이 어떤 이야기의 일부인지, 어떤 맥락에서 사용되었는지에 대한 추가 정보가 필요합니다. \n",
            "\n",
            "혹시 이 문장에 대한 더 자세한 정보를 알려주시면, 제가 더 정확하게 분석하고 이해하는 데 도움을\n",
            "TOTAL TIME ELAPSED: 23.75s\n",
            "--------------------------------------------------------------------------------\n",
            "This appears to be a nonsensical string of characters. It doesn't form a recognizable word or phrase in any language. \n",
            "\n",
            "It's likely a random collection of letters. \n",
            "\n",
            "Is there anything specific you were trying to communicate with this? Perhaps you were trying to create a code or a message?<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 6.33s\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_prompt = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "for test_case in test_cases:\n",
        "    tick()\n",
        "    output = sampler.sample(chat_prompt.format(prompt=test_case), max_new_tokens=MAX_NEW_TOKENS)\n",
        "    print(output)\n",
        "    tock()\n",
        "    print('-'*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3vQZukvvBAT"
      },
      "source": [
        "### With In-context Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNYHHRV7FtcB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대감께서 굳이 작고 굳이 굳치흣터지거니와.<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 42.74s\n",
            "--------------------------------------------------------------------------------\n",
            "금두겁이품의 드러뵈니 일정자 굳이 품에 붙어 있네.<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 43.40s\n",
            "--------------------------------------------------------------------------------\n",
            "이 밤 초팔일이 라이날 밤의 오수구룸이 집을 두루고 향냄이 동동 춤추며, 여성의 얼굴을 들고 드러와 김수영이 려니르.<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 46.50s\n",
            "--------------------------------------------------------------------------------\n",
            "길동이 길 너방에 말하지 말소셔이아희 필은낙양니샹셔집아지말소<end_of_turn>\n",
            "TOTAL TIME ELAPSED: 44.33s\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "in_context_learning_prompt = in_context_learning + \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "for test_case in test_cases:\n",
        "    tick()\n",
        "    output = sampler.sample(in_context_learning_prompt.format(prompt=test_case), max_new_tokens=MAX_NEW_TOKENS)\n",
        "    print(output)\n",
        "    tock()\n",
        "    print('-'*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9KHzSwqwAbr"
      },
      "source": [
        "For your reference, please see the following text, which has been translated by a human.\n",
        "\n",
        "---\n",
        "\n",
        "Early Hangul :\n",
        "`금두겁이품의드러뵈니일졍ᄌᆡᄌᆞᄅᆞᆯ나흐리로다ᄒᆞ더니과연그ᄃᆞᆯ부터잉ᄐᆡᄒᆞ여십삭이차니`\n",
        "\n",
        "Result via in-context Learning : `금두겁이품의 드러뵈니 일정자 굳이 품에 붙어 있네.<end_of_turn>`\n",
        "\n",
        "Human translation :\n",
        "```\n",
        "금두꺼비가 품에 드는 게 보였으니 얼마 안 있어 자식을 낳을 것입니다.\n",
        "\n",
        "하였다. 과연 그 달부터 잉태하여 십삭이 차니\n",
        "```\n",
        "\n",
        "> Note: Korean text means, “I saw a golden toad in her arms, so it won’t be long before she gives birth to a child.” Indeed, she conceived from that month and was ten months old.\n",
        "\n",
        "---\n",
        "\n",
        "Early Hangul : `이ᄯᆡᄂᆞᆫᄉᆞ월초팔일이라이날밤의오ᄉᆡᆨ구룸이집을두루고향ᄂᆡ진동ᄒᆞ며션녀ᄒᆞᆫᄡᅣᆼ이촉을들고드러와김ᄉᆡᆼᄃᆞ려니르ᄃᆡ`\n",
        "\n",
        "Result via in-context Learning : `이 밤 초팔일이 라이날 밤의 오수구룸이 집을 두루고 향냄이 동동 춤추며, 여성의 얼굴을 들고 드러와 김수영이 려니르.<end_of_turn>`\n",
        "\n",
        "Human translation :\n",
        "```\n",
        "이 때는 사월 초파일이었다. 이날 밤에 오색구름이 집을 두르고 향내 진동하며 선녀 한 쌍이 촉을 들고 들어와 김생더러 말하기를,\n",
        "```\n",
        "\n",
        "> Note: Korean text means, At this time, it was the 8th of April. On this night, with five-colored clouds surrounding the house and the scent of incense vibrating, a pair of fairies came in holding candles and said to Kim Saeng,\n",
        "\n",
        "Although the translation is not flawless, it provides a decent initial draft. The results are remarkable, considering that the datasets are limited to a single book. Enhancing the diversity of data sources will likely improve the translation quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxTbO1U2L3mm"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Why this works? Because the model is leveraging their extensive pre-training knowledge and the power of the transformer architecture to recognize patterns, infer concepts, and adapt their behavior to new tasks presented directly within the input context.\n",
        "\n",
        "When deciding between \"Fine-tuning\" and \"In-context learning\" with LLM, consider following factors:\n",
        "\n",
        "### Use Fine-tuning When:\n",
        "\n",
        "* **High Performance and Accuracy are Crucial:** For tasks requiring the best possible performance and accuracy in a specific domain, fine-tuning is generally preferred. It allows the model to deeply adapt its parameters to the nuances of your data.\n",
        "* **Domain-Specific Tasks:** If your application focuses on a narrow domain with specific vocabulary, style, or knowledge (e.g., legal documents, medical records, financial reports), fine-tuning can significantly improve the model's understanding and output quality.\n",
        "* **Consistent Output Format is Needed:** Fine-tuning can enforce a specific output format or structure more reliably than in-context learning.\n",
        "* **Cost-Effective Inference in the Long Run:** While fine-tuning requires upfront computational cost and labeled data, the resulting specialized model can often be smaller and more efficient for inference, leading to lower long-term costs, especially for high-volume applications.\n",
        "\n",
        "### Use In-Context Learning When:\n",
        "\n",
        "* **Rapid Prototyping and Experimentation:** In-context learning is excellent for quickly testing the capabilities of an LLM on a new task without the time and resource investment of fine-tuning.\n",
        "* **Flexibility and Task Switching:** If your application needs to handle a wide variety of tasks or adapt to changing requirements on the fly, in-context learning allows you to guide the model with different prompts and examples without retraining.\n",
        "* **Limited or No Labeled Data:** When you don't have a large, labeled dataset for your specific task, in-context learning can be a viable alternative by providing a few relevant examples directly in the prompt.\n",
        "* **Utilizing General Capabilities of Large Models:** If the task can be reasonably accomplished by leveraging the broad knowledge and general language understanding of a powerful pre-trained model with well-crafted prompts.\n",
        "\n",
        "### Hybrid Approach:\n",
        "\n",
        "It's also possible to combine both techniques. You could fine-tune a model on a broader domain and then use in-context learning with specific examples to further refine its behavior for particular sub-tasks within that domain.\n",
        "\n",
        "### In summary:\n",
        "\n",
        "* **Fine-tuning** is for **specialization, high performance, and long-term efficiency** when you have sufficient labeled data and a well-defined task.\n",
        "* **In-context learning** is for **flexibility, rapid experimentation, and handling diverse or low-data scenarios** by leveraging the general capabilities of large models through effective prompting.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[Gemma_3]In-context_Learning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
