# Gemma

| **Inference and serving**                                                                                            |                                                                                                                                                                                         |
| :------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Keras_Gemma_2_Quickstart.ipynb](Keras_Gemma_2_Quickstart.ipynb)                                               | Gemma 2 pre-trained 9B model quickstart tutorial with Keras.                                                                                                                            |
| [Keras_Gemma_2_Quickstart_Chat.ipynb](Keras_Gemma_2_Quickstart_Chat.ipynb)                                     | Gemma 2 instruction-tuned 9B model quickstart tutorial with Keras. Referenced in this [blog](https://developers.googleblog.com/en/fine-tuning-gemma-2-with-keras-hugging-face-update/). |
| [Gemma inference with Flax/NNX](https://flax.readthedocs.io/en/latest/guides/gemma.html)                             | Gemma 1 inference with Flax/NNX framework (linking to Flax documentation)                                                                                                               |
| [Chat_and_distributed_pirate_tuning.ipynb](Chat_and_distributed_pirate_tuning.ipynb)                           | Chat with Gemma 7B and finetune it so that it generates responses in pirates' tone.                                                                                                     |
| [gemma_inference_on_tpu.ipynb](gemma_inference_on_tpu.ipynb)                                                   | Basic inference of Gemma with JAX/Flax on TPU.                                                                                                                                          |
| [gemma_data_parallel_inference_in_jax_tpu.ipynb](gemma_data_parallel_inference_in_jax_tpu.ipynb)               | Parallel inference of Gemma with JAX/Flax on TPU.                                                                                                                                       |
| [Gemma_Basics_with_HF.ipynb](Gemma_Basics_with_HF.ipynb)                                                       | Load, run, finetune and deploy Gemma using [Hugging Face](https://huggingface.co/).                                                                                                     |
| [Gemma_with_Langfun_and_LlamaCpp.ipynb](Gemma_with_Langfun_and_LlamaCpp.ipynb)                                 | Leverage [Langfun](https://github.com/google/langfun) to seamlessly integrate natural language with programming using Gemma 2 and [LlamaCpp](https://github.com/ggerganov/llama.cpp).   |
| [Gemma_with_Langfun_and_LlamaCpp_Python_Bindings.ipynb](Gemma_with_Langfun_and_LlamaCpp_Python_Bindings.ipynb) | Leverage [Langfun](https://github.com/google/langfun) for smooth language-program interaction with Gemma 2 and [llama-cpp-python](https://github.com/abetlen/llama-cpp-python).         |
| [Guess_the_word.ipynb](Guess_the_word.ipynb)                                                                   | Play a word guessing game with Gemma using Keras.                                                                                                                                       |
| [Game_Design_Brainstorming.ipynb](Game_Design_Brainstorming.ipynb)                                             | Use Gemma to brainstorm ideas during game design using Keras.                                                                                                                           |
| [Translator_of_Old_Korean_Literature.ipynb](Translator_of_Old_Korean_Literature.ipynb)                         | Use Gemma to translate old Korean literature using Keras.                                                                                                                               |
| [Gemma2_on_Groq.ipynb](Gemma2_on_Groq.ipynb)                                                                   | Leverage the free Gemma 2 9B IT model hosted on [Groq](https://groq.com/) (super fast speed).                                                                                           |
| [Run_with_Ollama.ipynb](Run_with_Ollama.ipynb)                                                                 | Run Gemma models using [Ollama](https://www.ollama.com/).  
| [Run_with_Ollama_Python.ipynb](Run_with_Ollama_Python.ipynb)                                                   | Run Gemma models using [Ollama Python library](https://github.com/ollama/ollama-python).                                                                                                                              |
| [Using_Gemma_with_Llamafile.ipynb](Using_Gemma_with_Llamafile.ipynb)                                           | Run Gemma models using [Llamafile](https://github.com/Mozilla-Ocho/llamafile/).                                                                                                         |
| [Using_Gemma_with_LlamaCpp.ipynb](Using_Gemma_with_LlamaCpp.ipynb)                                             | Run Gemma models using [LlamaCpp](https://github.com/abetlen/llama-cpp-python/).                                                                                                        |
| [Using_Gemma_with_LocalGemma.ipynb](Using_Gemma_with_LocalGemma.ipynb)                                         | Run Gemma models using [Local Gemma](https://github.com/huggingface/local-gemma/).                                                                                                      |
| [Using_Gemma_with_mistral_rs.ipynb](Using_Gemma_with_mistral_rs.ipynb)                                         | Run Gemma models using [mistral.rs](https://github.com/EricLBuehler/mistral.rs/).                                                                                                       |
| [Using_Gemini_and_Gemma_with_RouteLLM.ipynb](Using_Gemini_and_Gemma_with_RouteLLM.ipynb)                       | Route Gemma and Gemini models using [RouteLLM](https://github.com/lm-sys/RouteLLM/).                                                                                                    |
| [Using_Gemma_with_SGLang.ipynb](Using_Gemma_with_SGLang.ipynb)                                                 | Run Gemma models using [SGLang](https://github.com/sgl-project/sglang/).                                                                                                                |
| [Using_Gemma_with_Xinference.ipynb](Using_Gemma_with_Xinference.ipynb)                                         | Run Gemma models using [Xinference](https://github.com/xorbitsai/inference/).                                                                                                           |
| [Constrained_generation_with_Gemma.ipynb](Constrained_generation_with_Gemma.ipynb)                             | Constrained generation with Gemma models using [LlamaCpp](https://github.com/abetlen/llama-cpp-python/) and [Guidance](https://github.com/guidance-ai/guidance/tree/main/).             |
| [Integrate_with_Mesop.ipynb](Integrate_with_Mesop.ipynb)                                                       | Integrate Gemma with [Google Mesop](https://google.github.io/mesop/).                                                                                                                   |
| [Integrate_with_OneTwo.ipynb](Integrate_with_OneTwo.ipynb)                                                     | Integrate Gemma with [Google OneTwo](https://github.com/google-deepmind/onetwo).                                                                                                        |
| [Deploy_with_vLLM.ipynb](Deploy_with_vLLM.ipynb)                                                               | Deploy a Gemma model using [vLLM](https://github.com/vllm-project/vllm).                                                                                                                |
| [Deploy_Gemma_in_Vertex_AI.ipynb](Deploy_Gemma_in_Vertex_AI.ipynb)                                             | Deploy a Gemma model using [Vertex AI](https://cloud.google.com/vertex-ai).                                                                                                             |
| [Gemma_Gradio_Chatbot.ipynb](Gemma_Gradio_Chatbot.ipynb)                                                       | Building a Chatbot with Gemma and Gradio                                                                                                                                                |
| [Synthetic_data_generation_with_Gemma_2.ipynb](Synthetic_data_generation_with_Gemma_2.ipynb)                   | Synthetic data generation with Gemma 2                                                                                                                                                  |
| **Prompting**                                                                                                        |                                                                                                                                                                                         |
| [Prompt_chaining.ipynb](Prompt_chaining.ipynb)                                                                 | Illustrate prompt chaining and iterative generation with Gemma.                                                                                                                         |
| [LangChain_chaining.ipynb](LangChain_chaining.ipynb)                                                           | Illustrate LangChain chaining  with Gemma.                                                                                                                                              |
| [Advanced_Prompting_Techniques.ipynb](Advanced_Prompting_Techniques.ipynb)                                     | Illustrate advanced prompting techniques with Gemma.                                                                                                                                    |
| **RAG**                                                                                                              |                                                                                                                                                                                         |
| [RAG_with_ChromaDB.ipynb](RAG_with_ChromaDB.ipynb)                                                             | Build a Retrieval Augmented Generation (RAG) system with Gemma using [ChromaDB](https://www.trychroma.com/) and [Hugging Face](https://huggingface.co/).                                |
| [Minimal_RAG.ipynb](Minimal_RAG.ipynb)                                                                         | Minimal example of building a RAG system with Gemma using [Google UniSim](https://github.com/google/unisim) and [Hugging Face](https://huggingface.co/).                                |
| [RAG_PDF_Search_in_multiple_documents_on_Colab.ipynb](RAG_PDF_Search_in_multiple_documents_on_Colab.ipynb)     | RAG PDF Search in multiple documents using Gemma 2 2B on Google Colab.                                                                                                                  |
| [Using_Gemma_with_LangChain.ipynb](Using_Gemma_with_LangChain.ipynb)                                           | Examples to demonstrate using Gemma with [LangChain](https://www.langchain.com/).                                                                                                       |
| [Using_Gemma_with_Elasticsearch_and_LangChain.ipynb](Using_Gemma_with_Elasticsearch_and_LangChain.ipynb)       | Example to demonstrate using Gemma with [Elasticsearch](https://www.elastic.co/elasticsearch/), [Ollama](https://www.ollama.com/) and [LangChain](https://www.langchain.com/).          |
| [Gemma_with_Firebase_Genkit_and_Ollama.ipynb](Gemma_with_Firebase_Genkit_and_Ollama.ipynb)                     | Example to demonstrate using Gemma with [Firebase Genkit](https://firebase.google.com/docs/genkit/) and [Ollama](https://www.ollama.com/)                                               |
| [Gemma_RAG_LlamaIndex.ipynb](Gemma_RAG_LlamaIndex.ipynb)                                                       | RAG example with [LlamaIndex](https://www.llamaindex.ai/) using Gemma.                                                                                                                  |
| **Finetuning**                                                                                                       |                                                                                                                                                                                         |
| [Finetune_with_Axolotl.ipynb](Finetune_with_Axolotl.ipynb)                                                     | Finetune Gemma using [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl).                                                                                                    |
| [Finetune_with_XTuner.ipynb](Finetune_with_XTuner.ipynb)                                                       | Finetune Gemma using [XTuner](https://github.com/InternLM/xtuner).                                                                                                                      |
| [Finetune_with_LLaMA_Factory.ipynb](Finetune_with_LLaMA_Factory.ipynb)                                         | Finetune Gemma using [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).                                                                                                         |
| [Finetune_with_Torch_XLA.ipynb](Finetune_with_Torch_XLA.ipynb)                                                 | Finetune Gemma using [PyTorch/XLA](https://github.com/pytorch/xla).                                                                                                                     |
| [Finetune_with_JORA.ipynb](Finetune_with_JORA.ipynb)                                                           | Finetune Gemma using [JORA](https://github.com/aniquetahir/JORA).                                                                                                                       |
| [Finetune_with_Unsloth.ipynb](Finetune_with_Unsloth.ipynb)                                                     | Finetune Gemma using [Unsloth](https://unsloth.ai/blog/gemma).                                                                                                                          |
| [Finetune_with_LitGPT.ipynb](Finetune_with_LitGPT.ipynb)                                                       | Finetune Gemma using [LitGPT](https://github.com/Lightning-AI/litgpt).                                                                                                                  |
| [Finetune_with_CALM.ipynb](Finetune_with_CALM.ipynb)                                                           | Finetune Gemma using [CALM](https://github.com/google-deepmind/calm).                                                                                                                   |
| [Finetuning_Gemma_for_Function_Calling.ipynb](Finetuning_Gemma_for_Function_Calling.ipynb)                     | Finetuning Gemma for Function Calling using [PyTorch/XLA](https://github.com/pytorch/xla).                                                                                              |
| [Custom_Vocabulary.ipynb](Custom_Vocabulary.ipynb)                                                             | Demonstrate how to use a custom vocabulary "&lt;unused[0-98]&gt;" tokens in Gemma.                                                                                                      |
| **Alignment**                                                                                                        |                                                                                                                                                                                         |
| [Aligning_DPO_Gemma_2b_it.ipynb](Aligning_DPO_Gemma_2b_it.ipynb)                                               | Demonstrate how to align a Gemma model using DPO (Direct Preference Optimization) with [Hugging Face TRL](https://huggingface.co/docs/trl/en/index).                                    |
| **Evaluation**                                                                                                       |                                                                                                                                                                                         |
| [Gemma_evaluation.ipynb](Gemma_evaluation.ipynb)                                                               | Demonstrate how to use Eleuther AI's LM evaluation harness to perform model evaluation on Gemma.                                                                                        |
